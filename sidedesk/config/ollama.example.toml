[ollama]
host = "http://localhost:11434"
default_model = "llama3.2"
temperature = 0.7
context_window = 4096

[vector_store]
persist_directory = "~/.sidedesk/embeddings"
collection_name = "sidedesk_knowledge"
embedding_model = "nomic-embed-text"

[mcp]
enabled_servers = [
    "filesystem",
    "terminal",
    "git"
]

[ai]
system_prompt = "You are a helpful AI assistant in SideDesk."
max_history_turns = 10
stream_responses = true
